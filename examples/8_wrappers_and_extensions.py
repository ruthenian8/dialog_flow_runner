import asyncio
import json
import logging
import random
from datetime import datetime

from df_engine.core import Actor

from df_runner import Pipeline, ComponentExecutionState, GlobalWrapperType, WrapperRuntimeInfo, ServiceRuntimeInfo
from examples._utils import SCRIPT

logging.basicConfig(level="INFO")
logger = logging.getLogger(__name__)

"""
The following example shows how pipeline can be extended by global wrappers and custom functions.

Pipeline functionality can be extended by global wrappers.
Global wrappers are special wrappers that are called on some stages of pipeline execution.
There are 4 types of global wrappers:
    `BEFORE_ALL` - is called before pipeline execution
    `BEFORE` - is called before each service and service group execution
    `AFTER` - is called after each service and service group execution
    `AFTER_ALL` - is called after pipeline execution
Global wrappers have the same signature as regular wrappers.
Actually `BEFORE_ALL` and `AFTER_ALL` are attached to root service group named 'pipeline', so they return its runtime info

All wrappers warnings (see example â„–7) are applicable to global wrappers.
Pipeline `add_global_wrapper` function is used to register global wrappers. It accepts following arguments:
    `global_wrapper_type` (required) - a GlobalWrapperType instance, indicates wrapper type to add
    `wrapper` (required) - the wrapper function itself
    `whitelist` - an optional list of paths, if it's not None the wrapper will be applied to specified pipeline components only
    `blacklist` - an optional list of paths, if it's not None the wrapper will be applied to all pipeline components except specified

Here basic functionality of `df-node-stats` library is emulated.
Information about pipeline component execution time and result is collected and printed to info log after pipeline execution.
Pipeline consists of actor and 25 `long_service`s that run random amount of time between 0 and 5 seconds.
"""


actor = Actor(
    SCRIPT,
    start_label=("greeting_flow", "start_node"),
    fallback_label=("greeting_flow", "fallback_node"),
)

start_times = dict()  # Place to temporarily store service start times
pipeline_info = dict()  # Pipeline information storage


def before_all(_, __, info: WrapperRuntimeInfo):
    global start_times, pipeline_info
    now = datetime.now()
    pipeline_info = {"start_time": now}
    start_times = {info["component"]["path"]: now}


def before(_, __, info: WrapperRuntimeInfo):
    start_times.update({info["component"]["path"]: datetime.now()})


def after(_, __, info: WrapperRuntimeInfo):
    start_time = start_times[info["component"]["path"]]
    pipeline_info.update(
        {
            f"{info['component']['path']}_duration": datetime.now() - start_time,
            f"{info['component']['path']}_success": info["component"]["execution_state"].get(
                info["component"]["path"], ComponentExecutionState.NOT_RUN.name
            ),
        }
    )


def after_all(_, __, info: WrapperRuntimeInfo):
    pipeline_info.update({f"total_time": datetime.now() - start_times[info["component"]["path"]]})
    logger.info(f"Pipeline stats:\n{json.dumps(pipeline_info, indent=4, default=str)}")


async def long_service(_, __, info: ServiceRuntimeInfo):
    timeout = random.randint(0, 5)
    logger.info(f"Service {info['name']} is going to sleep for {timeout} seconds.")
    await asyncio.sleep(timeout)


pipeline_dict = {
    "components": [
        [long_service for _ in range(0, 25)],
        actor,
    ],
}


pipeline = Pipeline(**pipeline_dict)

pipeline.add_global_wrapper(GlobalWrapperType.BEFORE_ALL, before_all)
pipeline.add_global_wrapper(GlobalWrapperType.BEFORE, before)
pipeline.add_global_wrapper(GlobalWrapperType.AFTER, after)
pipeline.add_global_wrapper(GlobalWrapperType.AFTER_ALL, after_all)

if __name__ == "__main__":
    pipeline.run()
